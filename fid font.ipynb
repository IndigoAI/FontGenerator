{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "actual-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import Dataset\n",
    "from model import Generator, Discriminator, CXLoss\n",
    "from vgg_cx import VGG19_CX\n",
    "# from wandb_config import API_KEY\n",
    "from calculate_fid import calculate_fid\n",
    "from inception import fid_inception_v3\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim import Adam\n",
    "from torch.utils import data\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# import wandb\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developed-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr2FontLearner(pl.LightningModule):\n",
    "    def __init__(self, attr_emb, n_unsupervised, gen_params, discr_params, optim_params, lambds):\n",
    "        super().__init__()\n",
    "        self.n_attr = gen_params['n_attr']\n",
    "        self.return_attr_D = discr_params['return_attr']\n",
    "\n",
    "        self.G = Generator(**gen_params)\n",
    "        self.D = Discriminator(**discr_params)\n",
    "        self.optim_params = optim_params\n",
    "\n",
    "        # attribute: N x 37 -> N x 37 x 64\n",
    "        self.attr_emb = nn.Embedding(self.n_attr, 64)\n",
    "        # n_unsupervised fonts + 1 dummy id (for supervised)\n",
    "        self.font_emb = nn.Embedding(n_unsupervised + 1, self.n_attr)  # attribute intensity\n",
    "\n",
    "        self.lambd_adv = lambds['lambd_adv']\n",
    "        self.lambd_pixel = lambds['lambd_pixel']\n",
    "        self.lambd_char = lambds['lambd_char']\n",
    "        self.lambd_cx = lambds['lamdb_cx']\n",
    "        self.lambd_attr = lambds['lamdb_attr']\n",
    "\n",
    "        self.gan_loss_fn = nn.MSELoss()\n",
    "        self.pixel_loss_fn = nn.L1Loss()\n",
    "        self.char_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.cx_loss_fn = CXLoss(sigma=0.5)\n",
    "        self.attr_loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.vgg19 = VGG19_CX().to(device)\n",
    "        self.vgg19.load_model('vgg19-dcbb9e9d.pth')\n",
    "        self.vgg19.eval()\n",
    "        self.vgg_layers = ['conv3_3', 'conv4_2']\n",
    "\n",
    "        self.sample_val = None\n",
    "\n",
    "    def forward(self, src_image, src_style, delta_emb, delta_attr_emb):\n",
    "        return self.G(src_image, src_style, delta_emb, delta_attr_emb)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        src_image = batch['src_image']\n",
    "        src_char = batch['src_char']\n",
    "        src_attr = batch['src_attribute']\n",
    "        src_style = batch['src_style']\n",
    "        src_label = batch['src_label'].unsqueeze(-1)\n",
    "        src_emb = batch['src_embed']\n",
    "\n",
    "        trg_image = batch['trg_image']\n",
    "        trg_attr = batch['trg_attribute']\n",
    "        trg_label = batch['trg_label'].unsqueeze(-1)\n",
    "        trg_emb = batch['trg_embed']\n",
    "\n",
    "        # numbers from 0 to 36\n",
    "        attr_ids = torch.tensor([i for i in range(self.n_attr)]).to(device)\n",
    "        attr_ids = attr_ids.repeat(len(src_image), 1)\n",
    "\n",
    "        # feature embeddings bs x 37 x emb_size\n",
    "        src_attr_emb = self.attr_emb(attr_ids)\n",
    "        trg_attr_emb = self.attr_emb(attr_ids)\n",
    "\n",
    "        # font embeddings bs x 37\n",
    "        src_emb = self.font_emb(src_emb)\n",
    "        src_emb = torch.sigmoid(3 * src_emb)      # why 3 ????\n",
    "        trg_emb = self.font_emb(trg_emb)\n",
    "        trg_emb = torch.sigmoid(3 * trg_emb)      # why 3 ????\n",
    "\n",
    "        # if sup - use initial emb, if unsup - use learned embs\n",
    "        src_unsup_emb = src_label * src_attr + (1 - src_label) * src_emb\n",
    "        trg_unsup_emb = trg_label * trg_attr + (1 - trg_label) * trg_emb\n",
    "\n",
    "        # for visual style transformer\n",
    "        delta_emb = trg_unsup_emb - src_unsup_emb\n",
    "\n",
    "        # for AAM\n",
    "        src_unsup_emb = src_unsup_emb.unsqueeze(-1)\n",
    "        trg_unsup_emb = trg_unsup_emb.unsqueeze(-1)\n",
    "        src_attr_embd = src_unsup_emb * src_attr_emb\n",
    "        trg_attr_embd = trg_unsup_emb * trg_attr_emb\n",
    "        delta_attr_emb = trg_attr_embd - src_attr_embd\n",
    "\n",
    "        # forward G\n",
    "        if optimizer_idx == 0:\n",
    "            trg_fake, src_logits = self(src_image, src_style, delta_emb, delta_attr_emb)\n",
    "            pred_fake, src_attr_pred, trg_attr_pred = self.D(src_image, trg_fake, trg_unsup_emb)\n",
    "\n",
    "            adv_loss = self.lambd_adv * self.gan_loss_fn(pred_fake, torch.ones_like(pred_fake))\n",
    "            pixel_loss = self.lambd_pixel * self.pixel_loss_fn(trg_fake, trg_image)\n",
    "            char_loss = self.lambd_char * self.char_loss_fn(src_logits, src_char - 10)  # src_char.shape = bs\n",
    "            attr_loss = self.lambd_attr * (self.attr_loss_fn(src_unsup_emb.squeeze(), src_attr_pred.double()) +\n",
    "                                           self.attr_loss_fn(trg_unsup_emb.squeeze(), trg_attr_pred.double()))\n",
    "\n",
    "            cx_loss = torch.zeros(1).to(device)\n",
    "            if self.lambd_cx > 0:\n",
    "                vgg_trg_fake = self.vgg19(trg_fake)\n",
    "                vgg_trg_img = self.vgg19(trg_image)\n",
    "\n",
    "                for l in self.vgg_layers:\n",
    "                    cx = self.cx_loss_fn(vgg_trg_img[l], vgg_trg_fake[l])\n",
    "                    cx_loss += cx * self.lambd_cx\n",
    "\n",
    "            loss_G = adv_loss + pixel_loss + char_loss + attr_loss + cx_loss\n",
    "            self.logger.log_metrics({'train_g_step_loss': loss_G.item(),\n",
    "                                     'train_adv_g_loss': adv_loss.item(),\n",
    "                                     'train_pixel_loss': pixel_loss.item(),\n",
    "                                     'train_char_loss': char_loss.item(),\n",
    "                                     'train_loss_cx': cx_loss.item(),\n",
    "                                     'train_attr_g_loss': attr_loss.item()})\n",
    "            return {'loss': loss_G}\n",
    "\n",
    "        # forward D\n",
    "        elif optimizer_idx == 1:\n",
    "            with torch.no_grad():\n",
    "                trg_fake, _ = self(src_image, src_style, delta_emb, delta_attr_emb)\n",
    "            pred_real, src_real_attr_pred, trg_real_attr_pred = self.D(src_image, trg_image, trg_unsup_emb.detach())\n",
    "            pred_fake, src_fake_attr_pred, trg_fake_attr_pred = self.D(src_image, trg_fake, trg_unsup_emb.detach())\n",
    "\n",
    "            loss_real = self.gan_loss_fn(pred_real, torch.ones_like(pred_real))\n",
    "            loss_fake = self.gan_loss_fn(pred_fake, torch.zeros_like(pred_real))\n",
    "\n",
    "            attr_loss = torch.zeros(1).to(device)\n",
    "            if self.return_attr_D:\n",
    "                attr_loss = self.lambd_attr * (self.attr_loss_fn(src_unsup_emb.squeeze(), src_real_attr_pred.double()) +\n",
    "                                               self.attr_loss_fn(trg_unsup_emb.squeeze(), trg_real_attr_pred.double()) +\n",
    "                                               self.attr_loss_fn(src_unsup_emb.squeeze(), src_fake_attr_pred.double()) +\n",
    "                                               self.attr_loss_fn(trg_unsup_emb.squeeze(), trg_fake_attr_pred.double()))\n",
    "\n",
    "            adv_loss = loss_real + loss_fake\n",
    "            loss_D = adv_loss + attr_loss\n",
    "            self.logger.log_metrics({'train_d_step_loss': loss_D,\n",
    "                                     'train_adv_d_loss': adv_loss,\n",
    "                                     'train_attr_d_loss': attr_loss})\n",
    "            return {'loss': loss_D}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_g_loss = torch.stack([x['loss'] for x in outputs[0]]).mean()\n",
    "        avg_d_loss = torch.stack([x['loss'] for x in outputs[1]]).mean()\n",
    "\n",
    "        self.logger.log_metrics({'train_g_epoch_loss': avg_g_loss,\n",
    "                                 'train_d_epoch_loss': avg_d_loss,\n",
    "                                 'epoch': self.current_epoch})\n",
    "\n",
    "    def validation_step(self, batch, *args):\n",
    "        src_image = batch['src_image']\n",
    "        src_style = batch['src_style']\n",
    "        src_emb = batch['src_embed']\n",
    "\n",
    "        trg_image = batch['trg_image']\n",
    "        trg_attr = batch['trg_attribute']\n",
    "\n",
    "        attr_ids = torch.tensor([i for i in range(self.n_attr)]).to(device)\n",
    "        attr_ids = attr_ids.repeat(len(src_image), 1)\n",
    "\n",
    "        src_attr_emb = self.attr_emb(attr_ids)\n",
    "        trg_attr_emb = self.attr_emb(attr_ids)\n",
    "\n",
    "        # source from unsup - use unsup emb\n",
    "        src_unsup_emb = self.font_emb(src_emb)\n",
    "        src_unsup_emb = torch.sigmoid(3 * src_unsup_emb)  # why 3 ????\n",
    "\n",
    "        # VST\n",
    "        delta_emb = trg_attr - src_unsup_emb\n",
    "\n",
    "        # AAM\n",
    "        src_attr_embd = src_unsup_emb.unsqueeze(-1) * src_attr_emb\n",
    "        trg_attr_embd = trg_attr.unsqueeze(-1) * trg_attr_emb\n",
    "        delta_attr_emb = trg_attr_embd - src_attr_embd\n",
    "\n",
    "        trg_fake, _ = self(src_image, src_style, delta_emb, delta_attr_emb)\n",
    "        loss = self.pixel_loss_fn(trg_fake, trg_image)\n",
    "\n",
    "        if self.sample_val is None:\n",
    "            self.sample_val = torch.cat((trg_image[:10], trg_fake[:10]), 0)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        grid_img = make_grid(self.sample_val, nrow=10)\n",
    "        self.logger.log_metrics({'val_loss': avg_loss,\n",
    "                                 'epoch': self.current_epoch,\n",
    "                                 'val imgs': [wandb.Image(grid_img)]})\n",
    "        self.sample_val = None\n",
    "        return {'val_loss': avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.optim_params['lr']\n",
    "        beta1 = self.optim_params['beta1']\n",
    "        beta2 = self.optim_params['beta2']\n",
    "        optimizer_G = Adam([\n",
    "            {'params': self.G.parameters()},\n",
    "            {'params': self.attr_emb.parameters(), 'lr': 1e-3},\n",
    "            {'params': self.font_emb.parameters(), 'lr': 1e-3}],\n",
    "            lr=lr, betas=(beta1, beta2))\n",
    "        optimizer_D = Adam(self.D.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        return [optimizer_G, optimizer_D], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_path = 'data/attributes.txt'\n",
    "image_path = 'data/image/'\n",
    "batch_size = 16\n",
    "epochs = 500\n",
    "\n",
    "attr_emb = 64\n",
    "n_unsupervised = 968\n",
    "\n",
    "gen_params = {\n",
    "    'in_channels': 3,\n",
    "    'style_out': 256,\n",
    "    'out_channels': 3,\n",
    "    'n_attr': 37,\n",
    "    'attention': True\n",
    "}\n",
    "\n",
    "discr_params = {\n",
    "    'in_channels': 3,\n",
    "    'attr_channels': 37,\n",
    "    'return_attr': True\n",
    "}\n",
    "\n",
    "optim_params = {\n",
    "    'lr': 2e-4,\n",
    "    'beta1': 0.5,\n",
    "    'beta2': 0.99\n",
    "}\n",
    "\n",
    "lambds = {\n",
    "    'lambd_adv': 5,\n",
    "    'lambd_pixel': 50,\n",
    "    'lambd_char': 3,\n",
    "    'lamdb_cx': 6,\n",
    "    'lamdb_attr': 20\n",
    "}\n",
    "\n",
    "model = Attr2FontLearner.load_from_checkpoint('epoch=108-val_loss=0.148.ckpt',\n",
    "                                                attr_embd=64,\n",
    "                                                n_unsupervised=n_unsupervised,\n",
    "                                                gen_params=gen_params,\n",
    "                                                discr_params=discr_params,\n",
    "                                                optim_params=optim_params,\n",
    "                                                lambds=lambds)\n",
    "\n",
    "train_dataset = Dataset(attribute_path, image_path,  mode='train')\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                               shuffle=True,\n",
    "                               drop_last=True,\n",
    "                               batch_size=batch_size)\n",
    "\n",
    "val_dataset = Dataset(attribute_path, image_path,  mode='test')\n",
    "val_loader = data.DataLoader(dataset=val_dataset,\n",
    "                             drop_last=True,\n",
    "                             batch_size=batch_size)\n",
    "\n",
    "classifier = fid_inception_v3()\n",
    "fid = calculate_fid(val_loader, model.G, classifier)\n",
    "print(fid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
